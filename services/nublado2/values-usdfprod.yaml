jupyterhub:

  hub:
    config:
      ServerApp:
        shutdown_no_activity_timeout: 432000

  cull:
    enabled: true
    users: false
    removeNamedServers: false
    timeout: 432000
    every: 300
    maxAge: 2160000
    
  ingress:
    hosts: ["usdf-rsp.slac.stanford.edu"]
    annotations:
      nginx.ingress.kubernetes.io/auth-signin: "https://usdf-rsp.slac.stanford.edu/login"
      nginx.ingress.kubernetes.io/auth-url: "https://usdf-rsp.slac.stanford.edu/auth?scope=exec:notebook&notebook=true"

config:
  base_url: "https://usdf-rsp.slac.stanford.edu"
  butler_secret_path: "secret/rubin/usdf-rsp/butler-secret"
  pull_secret_path: "secret/rubin/usdf-rsp/pull-secret"
  cachemachine_image_policy: "desired"

  lab_environment:
    PGPASSFILE: "/opt/lsst/software/jupyterlab/butler-secret/postgres-credentials.txt"
    AWS_SHARED_CREDENTIALS_FILE: "/opt/lsst/software/jupyterlab/butler-secret/aws-credentials.ini"
    DAF_BUTLER_REPOSITORY_INDEX: "s3://butler-us-central1-repo-locations/data-repos.yaml"
    S3_ENDPOINT_URL: "https://storage.googleapis.com"
    AUTO_REPO_URLS: https://github.com/lsst-sqre/system-test,https://github.com/rubin-dp0/tutorial-notebooks
    AUTO_REPO_BRANCH: prod
    AUTO_REPO_SPECS: https://github.com/lsst-sqre/system-test@prod,https://github.com/rubin-dp0/tutorial-notebooks@prod
    NO_ACTIVITY_TIMEOUT: "432000"
    CULL_KERNEL_IDLE_TIMEOUT: "432000"
    CULL_KERNEL_CONNECTED: "True"
    CULL_KERNEL_INTERVAL: "300"
    CULL_TERMINAL_INACTIVE_TIMEOUT: "432000"
    CULL_TERMINAL_INTERVAL: "300"
    http_proxy: http://sdfproxy.sdf.slac.stanford.edu:3128
    https_proxy: http://sdfproxy.sdf.slac.stanford.edu:3128
    no_proxy: hub.nublado2,.sdf.slac.stanford.edu,.slac.stanford.edu

  volumes:
    - name: home
      persistentVolumeClaim:
        claimName: sdf-home
    - name: sdf-group-rubin
      persistentVolumeClaim:
        claimName: sdf-group-rubin
    - name: sdf-data-rubin
      persistentVolumeClaim:
        claimName: sdf-data-rubin
    - name: fs-ddn-sdf-group-rubin
      persistentVolumeClaim:
        claimName: fs-ddn-sdf-group-rubin
    - name: fs-nfs
      persistentVolumeClaim:
        claimName: fs-nfs
#    - name: sdf-scratch
#      persistentVolumeClaim:
#        claimName: sdf-scratch
    #- name: sdf-group-lsst
    #  persistentVolumeClaim:
    #    claimName: sdf-group-lsst
  volume_mounts:
    - name: home
      mountPath: "/home/{{ user }}"
      #mountPath: "/home/{{user}}"
      subPath: "{{user[0]}}/{{user}}"
#    - name: sdf-group-rubin
#      mountPath: /datasets
#      subPath: datasets
    - name: sdf-group-rubin
      mountPath: /repo
      subPath: repo
    - name: sdf-group-rubin
      mountPath: /project
      subPath: g
    - name: sdf-group-rubin
      mountPath: /sdf/group/rubin
    - name: sdf-data-rubin
      mountPath: /sdf/data/rubin
#    - name: sdf-scratch
#      mountPath: /scratch
    - name: fs-ddn-sdf-group-rubin
      mountPath: /teststand
      subPath: lsstdata/offline/teststand
    - name: fs-ddn-sdf-group-rubin 
      mountPath: /instrument
      subPath: lsstdata/offline/instrument
    - name: fs-nfs
      mountPath: /fs/nfs
    - name: fs-ddn-sdf-group-rubin
      mountPath: /fs/ddn/sdf/group/rubin
    #- name: sdf-group-lsst
    #  mountPath: /sdf/group/lsst


  # Workaround to impose resource quotas at IDF
  user_resources_template: |
    - apiVersion: v1
      kind: Namespace
      metadata:
        name: "{{ user_namespace }}"
    - apiVersion: v1
      kind: ConfigMap
      metadata:
        name: group
        namespace: "{{ user_namespace }}"
      data:
        group: |
          root:x:0:
          bin:x:1:
          daemon:x:2:
          sys:x:3:
          adm:x:4:
          tty:x:5:
          disk:x:6:
          lp:x:7:
          mem:x:8:
          kmem:x:9:
          wheel:x:10:
          cdrom:x:11:
          mail:x:12:
          man:x:15:
          dialout:x:18:
          floppy:x:19:
          games:x:20:
          tape:x:33:
          video:x:39:
          ftp:x:50:
          lock:x:54:
          audio:x:63:
          nobody:x:99:
          users:x:100:
          utmp:x:22:
          utempter:x:35:
          input:x:999:
          systemd-journal:x:190:
          systemd-network:x:192:
          dbus:x:81:
          ssh_keys:x:998:
          tss:x:59:
          cgred:x:997:
          screen:x:84:
          provisionator:x:769:
          {{user}}:x:{{uid}}:{% for group in groups %}
          {{ group.name }}:x:{{ group.id }}:{{ user }}{% endfor %}
    - apiVersion: v1
      kind: ConfigMap
      metadata:
        name: gshadow
        namespace: "{{ user_namespace }}"
      data:
        gshadow: |
          root:!::
          bin:!::
          daemon:!::
          sys:!::
          adm:!::
          tty:!::
          disk:!::
          lp:!::
          mem:!::
          kmem:!::
          wheel:!::
          cdrom:!::
          mail:!::
          man:!::
          dialout:!::
          floppy:!::
          games:!::
          tape:!::
          video:!::
          ftp:!::
          lock:!::
          audio:!::
          nobody:!::
          users:!::
          utmp:!::
          utempter:!::
          input:!::
          systemd-journal:!::
          systemd-network:!::
          dbus:!::
          ssh_keys:!::
          tss:!::
          cgred:!::
          screen:!::
          provisionator:!::
          {{ user }}:!::{% for g in groups %}
          {{ g.name }}:!::{{ user }}{% endfor %}
    - apiVersion: v1
      kind: ConfigMap
      metadata:
        name: passwd
        namespace: "{{ user_namespace }}"
      data:
        passwd: |
          root:x:0:0:root:/root:/bin/bash
          bin:x:1:1:bin:/bin:/sbin/nologin
          daemon:x:2:2:daemon:/sbin:/sbin/nologin
          adm:x:3:4:adm:/var/adm:/sbin/nologin
          lp:x:4:7:lp:/var/spool/lpd:/sbin/nologin
          sync:x:5:0:sync:/sbin:/bin/sync
          shutdown:x:6:0:shutdown:/sbin:/sbin/shutdown
          halt:x:7:0:halt:/sbin:/sbin/halt
          mail:x:8:12:mail:/var/spool/mail:/sbin/nologin
          operator:x:11:0:operator:/root:/sbin/nologin
          games:x:12:100:games:/usr/games:/sbin/nologin
          ftp:x:14:50:FTP User:/var/ftp:/sbin/nologin
          nobody:x:99:99:Nobody:/:/sbin/nologin
          systemd-network:x:192:192:systemd Network Management:/:/sbin/nologin
          dbus:x:81:81:System message bus:/:/sbin/nologin
          tss:x:59:59:Account used by the trousers package to sandbox the tcsd daemon:/dev/null:/sbin/nologin
          provisionator:x:769:769:Lab provisioning user:/home/provisionator:/bin/bash
          {{ user }}:x:{{ uid }}:{{ uid }}::/home/{{ user }}:/bin/bash
    - apiVersion: v1
      kind: ConfigMap
      metadata:
        name: shadow
        namespace: "{{ user_namespace }}"
      data:
        shadow: |
          root:*:18000:0:99999:7:::
          bin:*:18000:0:99999:7:::
          daemon:*:18000:0:99999:7:::
          adm:*:18000:0:99999:7:::
          lp:*:18000:0:99999:7:::
          sync:*:18000:0:99999:7:::
          shutdown:*:18000:0:99999:7:::
          halt:*:18000:0:99999:7:::
          mail:*:18000:0:99999:7:::
          operator:*:18000:0:99999:7:::
          games:*:18000:0:99999:7:::
          ftp:*:18000:0:99999:7:::
          nobody:*:18000:0:99999:7:::
          systemd-network:*:18000:0:99999:7:::
          dbus:*:18000:0:99999:7:::
          lsst_lcl:*:18000:0:99999:7:::
          tss:*:18000:0:99999:7:::
          provisionator:*:18000:0:99999:7:::
          {{user}}:*:18000:0:99999:7:::

    - apiVersion: v1
      kind: ConfigMap
      metadata:
        name: dask
        namespace: "{{ user_namespace }}"
      data:
        dask_worker.yml: |
          {{ dask_yaml | indent(6) }}
    # When we break out the resources we should make this per-instance
    #  configurable.
    - apiVersion: v1
      kind: ConfigMap
      metadata:
        name: idds-config
        namespace: "{{ user_namespace }}"
      data:
        idds_cfg.client.template: |
          # Licensed under the Apache License, Version 2.0 (the "License");
          # You may not use this file except in compliance with the License.
          # You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0
          #
          # Authors:
          # - Wen Guan, <wen.guan@cern.ch>, 2020
          [common]
          # if logdir is configured, idds will write to idds.log in this directory.
          # else idds will go to stdout/stderr.
          # With supervisord, it's good to write to stdout/stderr, then supervisord can manage and rotate logs.
          # logdir = /var/log/idds
          loglevel = INFO
          [rest]
          host = https://iddsserver.cern.ch:443/idds
          #url_prefix = /idds
          #cacher_dir = /tmp
          cacher_dir = /data/idds
    - apiVersion: v1
      kind: ServiceAccount
      metadata:
        name: "{{ user }}-serviceaccount"
        namespace: "{{ user_namespace }}"
      imagePullSecrets:
      - name: pull-secret
    - apiVersion: rbac.authorization.k8s.io/v1
      kind: Role
      metadata:
        name: "{{ user }}-role"
        namespace: "{{ user_namespace }}"
      rules:
      # cf https://kubernetes.dask.org/en/latest/kubecluster.html
        - apiGroups: [""]
          resources: ["pods", "services"]
          verbs: ["create", "delete", "get", "list", "watch"]
        - apiGroups: [""]
          resources: ["pods/log"]
          verbs: ["get","list"]
        - apiGroups: ["policy"]
          resources: ["poddisruptionbudgets"]
          verbs: ["create", "delete", "get", "list", "watch"]
    - apiVersion: rbac.authorization.k8s.io/v1
      kind: RoleBinding
      metadata:
        name: "{{ user }}-rolebinding"
        namespace: "{{ user_namespace }}"
      roleRef:
        apiGroup: rbac.authorization.k8s.io
        kind: Role
        name: "{{ user }}-role"
      subjects:
        - kind: ServiceAccount
          name: "{{ user }}-serviceaccount"
          namespace: "{{ user_namespace }}"
    - apiVersion: ricoberger.de/v1alpha1
      kind: VaultSecret
      metadata:
        name: butler-secret
        namespace: "{{ user_namespace }}"
      spec:
        path: "{{ butler_secret_path }}"
        type: Opaque
    - apiVersion: ricoberger.de/v1alpha1
      kind: VaultSecret
      metadata:
        name: pull-secret
        namespace: "{{ user_namespace }}"
      spec:
        path: "{{ pull_secret_path }}"
        type: kubernetes.io/dockerconfigjson
    - apiVersion: v1
      kind: ResourceQuota
      metadata:
        name: user-quota
        namespace: "{{ user_namespace }}"
      spec:
        hard:
          limits.cpu: 9
          limits.memory: 27Gi
    - apiVersion: v1
      kind: PersistentVolumeClaim
      metadata:
        name: sdf-group-rubin
        namespace: "{{ user_namespace }}"
      spec:
        storageClassName: sdf-group-rubin
        accessModes:
          - ReadWriteMany
        resources:
          requests:
            storage: 1Gi
    - apiVersion: v1
      kind: PersistentVolumeClaim
      metadata:
        name: sdf-data-rubin
        namespace: "{{ user_namespace }}"
      spec:
        storageClassName: sdf-data-rubin
        accessModes:
          - ReadWriteMany
        resources:
          requests:
            storage: 1Gi
    - apiVersion: v1
      kind: PersistentVolumeClaim
      metadata:
        name: sdf-home
        namespace: "{{ user_namespace }}"
      spec:
        storageClassName: sdf-home
        accessModes:
          - ReadWriteMany
        resources:
          requests:
            storage: 1Gi
    - apiVersion: v1
      kind: PersistentVolumeClaim
      metadata:
        name: fs-nfs
        namespace: "{{ user_namespace }}"
      spec:
        storageClassName: fs-nfs
        accessModes:
          - ReadWriteMany
        resources:
          requests:
            storage: 1Gi
    - apiVersion: v1
      kind: PersistentVolumeClaim
      metadata:
        name: fs-ddn-sdf-group-rubin
        namespace: "{{ user_namespace }}"
      spec:
        storageClassName: fs-ddn-sdf-group-rubin
        accessModes:
          - ReadWriteMany
        resources:
          requests:
            storage: 1Gi
#    - apiVersion: v1
#      kind: PersistentVolumeClaim
#      metadata:
#        name: sdf-group-lsst
#        namespace: "{{ user_namespace }}"
#      spec:
#        storageClassName: sdf-group-lsst
#        accessModes:
#          - ReadWriteMany
#        resources:
#          requests:
#            storage: 1Gi

vault_secret_path: "secret/rubin/usdf-rsp/nublado2"

pull-secret:
  enabled: true
  path: "secret/rubin/usdf-rsp/pull-secret"
