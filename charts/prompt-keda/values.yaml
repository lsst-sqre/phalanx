# Default values for the prompt-keda scaled job.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.

image:
  # -- Image to use in the Prompt Processing deployment
  repository: ghcr.io/lsst-dm/prompt-service
  # -- Pull policy for the Prompt Processing image
  # @default -- `IfNotPresent` in prod, `Always` in dev.  Set to `IfNotPresent` for scale testing in dev.
  pullPolicy: IfNotPresent
  # -- Overrides the image tag whose default is the chart appVersion.
  tag: latest

worker:
  # -- The number of requests to process before rebooting a worker.
  # If 0, workers process requests indefinitely.
  restart: 0
  # -- When Kubernetes shuts down a pod, the time its workers have to abort processing and save intermediate results (seconds).
  grace_period: 45

instrument:
  # -- The "short" name of the instrument
  # @default -- None, must be set
  name: ""
  pipelines:
    # IMPORTANT: don't use flow-style mappings (i.e., {}) in pipelines specs
    # if the result (including any comments) is longer than 72 characters.
    # The config will get corrupted after template substitution.
    # Block-style mappings can have lines of any length.

    # -- YAML-formatted config describing which pipeline(s) should be run for which visits' raws.
    # Fields are still in flux; see [the source code](https://github.com/lsst-dm/prompt_processing/blob/main/python/activator/config.py) for examples.
    # @default -- None, must be set
    main: ""
    # -- YAML-formatted config describing which pipeline(s) should be run before which visits' raw arrival.
    # @default -- None, must be set
    preprocessing: ""
  # -- Skymap to use with the instrument
  skymap: ""
  # -- Number of arcseconds to pad the spatial region in preloading.
  preloadPadding: 30
  # -- URI to the shared repo used for pipeline inputs and outputs.
  # If `registry.centralRepoFile` is set, this URI points to a local redirect instead of the central repo itself.
  # @default -- None, must be set
  centralRepo: ""
  # -- Optional URI to a separate repo used for pipeline inputs.
  # If `registry.centralRepoFile` is set, this URI points to a local redirect instead of the central repo itself.
  # @default -- Matches `centralRepo`
  readRepo: ""
  # -- The average time to wait (in seconds) before retrying a failed connection to the shared repo.
  repoWait: 30
  # -- Dataset types to export.
  exportTypes: "- .*"

cache:
  # -- The default number of datasets of each type to keep.
  # The pipeline only needs one of most dataset types (one bias, one flat, etc.), so this is roughly the number of visits that fit in the cache.
  baseSize: 3

s3:
  # -- Bucket containing the incoming raw images
  # @default -- None, must be set
  imageBucket: ""
  # -- S3 endpoint containing `imageBucket`
  # @default -- None, must be set
  endpointUrl: ""
  # -- If set, define environment variables with S3 credentials from this application's Vault secret.
  auth_env: false
  # -- If set, get a S3 credential file from this application's Vault secret.
  # Must be true if `aws_profile` is set.
  cred_file_auth: true
  # -- If set, specify a S3 credential profile and `cred_file_auth` must be true.
  # If empty and `auth_env`=`false`, the `default` profile is used.
  aws_profile: ""
  # -- Set this to disable validation of S3 bucket names, allowing Ceph multi-tenant colon-separated names to be used.
  disableBucketValidation: 0
  # -- If set, configure S3 checksum options.
  checksum: WHEN_REQUIRED

# -- The URI to a microservice that maps image metadata to a file location.
# If empty, Prompt Processing does not use a microservice.
raw_microservice: ""

# -- The URI to the MPSky ephemerides service.
# Empty value is allowed, but its handling is undefined.
mpSky_service: ""

# -- The URI where IERS data has been pre-downloaded and cached for use by Prompt Processing.
# If empty, Prompt Processing does not try to update IERS data.
iers_cache: ""

imageNotifications:
  # -- Hostname and port of the Kafka provider
  # @default -- None, must be set
  kafkaClusterAddress: ""
  # -- Topic where raw image arrival notifications appear
  # @default -- None, must be set
  topic: ""
  # -- Kafka consumer offset reset setting for image arrival notifications
  consumerOffsetReset: "latest"
  # -- Timeout to wait after expected script completion for raw image arrival (seconds).
  imageTimeout: 20

apdb:
  # -- URL to a serialized APDB configuration, or the "label:" prefix followed
  # by the indexed name of such a config.
  # @default -- None, must be set
  config: ""

alerts:
  # -- Username for sending alerts to the alert stream
  username: ""
  # -- Server address for the alert stream
  server: ""
  # -- Topic name where alerts will be sent
  topic: ""

registry:
  # -- If set, this application's Vault secret must contain a `central_repo_file` key containing a remote Butler configuration, and `instrument.centralRepo` is the local path where this file is mounted.
  # If `instrument.readRepo` is also set, the secret must contain a `read_repo_file` key.
  centralRepoFile: false

# -- Requested logging levels in the format of [Middleware's \-\-log-level argument](https://pipelines.lsst.io/v/daily/modules/lsst.daf.butler/scripts/butler.html#cmdoption-butler-log-level).
# @default -- log prompt_processing at DEBUG, other LSST code at INFO, and third-party code at WARNING.
logLevel: ""

sasquatch:
  # -- Url of the Sasquatch proxy server to upload metrics to. Leave blank to disable upload.
  # This is a preliminary implementation of Sasquatch support, and this parameter may be deprecated
  # if we instead support `SasquatchDatastore` in the future.
  endpointUrl: ""
  # -- Namespace in the Sasquatch system with which to associate metrics.
  namespace: lsst.prompt
  # -- If set, this application's Vault secret must contain a `sasquatch_token` key containing the authentication token for `sasquatch.endpointUrl`.
  # Leave unset to attempt anonymous access.
  auth_env: true

sattle:
  # -- Base URI of the sattle service.  Leave blank if not used.
  uri_base: ""

keda:
  # -- Minimum number of replicas to start with.
  minReplicaCount: 3
  # -- Maximum number of replicas to scale to.
  maxReplicaCount: 10
  # -- Scaling algorithm
  scalingStrategy: eager
  # -- Polling interval for Keda to poll scalar for scaling determination.
  pollingInterval: 2
  # -- How many completed jobs should be kept available in Kubernetes.
  successfulJobsHistoryLimit: 25
  # -- How many failed jobs should be kept available in Kubernetes.
  failedJobsHistoryLimit: 25

  redisStreams:
    # -- Address of Redis Streams Cluster
    host: ""
    # -- Name of Redis Stream
    streamName: ""
    # -- Redis Consumer Group name
    consumerGroup: ""
    # -- The time to wait (in seconds) before retrying a stream connection failure.
    retry: 30
    # -- Time to wait for fanned out messages before spawning new pod.
    msgListenTimeout: 900
    # -- Lag count at which scaler triggers
    activationLagCount: "1"
    # -- Number of lagging entries in the consumer group, alternative to pendingEntriesCount scaler trigger
    lagCount: "1"
    # -- Number of entries in the Pending Entries List for the specified consumer group in the Redis Stream
    pendingEntriesCount: "1"
    # -- Maximum message age to process, in seconds.
    expiration: 3600

initializer:
  image:
    # -- Image to use for the PP initializer
    repository: ghcr.io/lsst-dm/prompt-init
    # Initializer and service must be consistent; pull policy and tag can't be set independently
  resources:
    # -- The cpu cores requested for the initializer.
    cpuRequest: 1
    # -- The maximum cpu cores for the initializer.
    cpuLimit: 1
    # -- The minimum memory to request for the initializer.
    memoryRequest: "512Mi"
    # -- The maximum memory limit for the initializer.
    memoryLimit: "1Gi"
  cron:
    # -- Time zone in which day_obs is computed.
    day_obs_tz: -12
    # -- Whether or not to pause daily initializer runs.
    # This makes it impossible to run Prompt Processing, but avoids repo clutter if the telescope is offline.
    suspend: false

  # -- Maximum time for a single attempt to initialize the central repo (seconds).
  timeout: 120
  # -- Maximum number of times to attempt initializing the central repo.
  # If the initializer fails, the PP service cannot run!
  retries: 6

  # -- Time after which to remove old initializer pods (seconds).
  cleanup_delay: 3600

  # -- Pod annotations for the init-output Job
  # @default -- See the `values.yaml` file.
  podAnnotations: {}

butler_writer:
  # -- If false, pipeline outputs will be written directly to the central repo.
  # If true, a Kafka message will be sent to a service to aggregate these writes instead.
  # The init job writes directly to the central repo regardless of this setting.
  enabled: false
  # -- Address of Kafka broker where prompt processing output events will be
  # written, for consumption by the Butler writer service.
  # @default -- None, must be set
  kafka_cluster: ""
  # -- Username for Kafka broker where prompt processing output events will be
  # written, for consumption by the Butler writer service.
  # @default -- None, must be set
  kafka_username: ""
  # -- Kafka topic that prompt processing output events will be written to,
  # for consumption by the Butler writer service.
  # @default -- None, must be set
  kafka_topic: ""
  # -- Root path where output dataset files will be written when transferring
  # back to the central repository.
  # @default -- None, must be set
  output_file_path: ""

# -- Override the base name for resources
nameOverride: ""

# -- Override the full name for resources (includes the release name)
fullnameOverride: "prompt-keda"

debug:
  # -- Whether or not pipeline outputs should be exported to the central repo.
  # This flag does not turn off APDB writes or alert generation; those must be handled at the pipeline level or by setting up an alternative destination.
  exportOutputs: true
  # -- Whether `dax_apdb` should run in debug mode and log metrics.
  monitorDaxApdb: false


# -- Kubernetes resource requests and limits
# @default -- See `values.yaml`
resources:
  requests:
    cpu: 1
    ephemeral-storage: 5Gi
    memory: 2Gi
  limits:
    cpu: 1
    ephemeral-storage: 5Gi
    memory: 8Gi

# -- Kubernetes YAML configs for extra container volume(s).
# Any volumes required by other config options are automatically handled by the Helm chart.
additionalVolumeMounts: []

# -- Pod annotations for the Prompt Processing Pod
podAnnotations: {
  prometheus.io/port: "8000",
  prometheus.io/scrape: "true"
}

# -- Affinity rules for the Prompt Processing Pod
affinity: {}

# -- Node selection rules for the Prompt Porcessing pod
nodeSelector: {}

# -- Tolerations for the Prompt Processing pod
tolerations: []
