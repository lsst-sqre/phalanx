# Default values for nublado2.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.

jupyterhub:
  hub:
    authenticatePrometheus: false
    image:
      name: lsstsqre/nublado2
      tag: "2.6.1"
    resources:
      limits:
        cpu: 900m
        memory: 1Gi  # Should support about 200 users
    config:
      Authenticator:
        enable_auth_state: true
      JupyterHub:
        authenticator_class: nublado2.auth.GafaelfawrAuthenticator
      ServerApp:
        shutdown_no_activity_timeout: 604800  # one week
    db:
      # Password comes from the nublado2-secret.
      type: "postgres"
      password: "true"
      url: "postgresql://jovyan@postgres.postgres/jupyterhub"
    containerSecurityContext:
      runAsUser: 768
      runAsGroup: 768
      allowPrivilegeEscalation: false
    baseUrl: "/nb"
    # Note: this has to match up with the kubernetes secret created by the
    # vault secret, and since you can't put templating in a values file, I'm
    # just setting the name here.
    existingSecret: "nublado2-secret"
    extraConfig:
      nublado.py: |
        import nublado2.hub_config
        nublado2.hub_config.HubConfig().configure(c)
    extraVolumes:
      - name: nublado-config
        configMap:
          name: nublado-config
      - name: nublado-gafaelfawr
        secret:
          secretName: gafaelfawr-token
    extraVolumeMounts:
      - name: nublado-config
        mountPath: /etc/jupyterhub/nublado_config.yaml
        subPath: nublado_config.yaml
      - name: nublado-gafaelfawr
        mountPath: /etc/keys/gafaelfawr-token
        subPath: token
    # We still have to use our own, enabled at the top level, which is
    #  similar but not identical.  This one still doesn't work, even if
    #  you explicitly enable port 8081 so the labs can talk to the Hub.
    networkPolicy:
      enabled: false
    loadRoles:
      self:
        scopes: ['admin:servers!user', 'read:metrics']
      server:
        scopes: ['inherit']  # Let server use API like user

  prePuller:
    continuous:
      enabled: false
    hook:
      enabled: false

  singleuser:
    cloudMetadata:
      blockWithIptables: false
    cmd: "/opt/lsst/software/jupyterlab/runlab.sh"
    defaultUrl: "/lab"
    extraAnnotations:
      argocd.argoproj.io/compare-options: 'IgnoreExtraneous'
      argocd.argoproj.io/sync-options: 'Prune=false'
    extraLabels:
      hub.jupyter.org/network-access-hub: 'true'
      argocd.argoproj.io/instance: 'nublado-users'
    storage:
      extraVolumes:
        - name: dask
          configMap:
            name: dask
        - name: idds-config
          configMap:
            name: idds-config
        - name: tmp
          emptyDir: {}
        - name: butler-secret
          secret:
            secretName: butler-secret
        - name: lab-environment
          configMap:
            defaultMode: 420
            name: lab-environment
        - name: passwd
          configMap:
            defaultMode: 420
            name: passwd
        - name: group
          configMap:
            defaultMode: 420
            name: group
      extraVolumeMounts:
        - name: dask
          mountPath: /etc/dask
        - name: idds-config
          mountPath: /opt/lsst/software/jupyterlab/panda
        - name: tmp
          mountPath: /tmp
        - name: butler-secret
          mountPath: /opt/lsst/software/jupyterlab/butler-secret
        - name: lab-environment
          mountPath: /opt/lsst/software/jupyterlab/environment
        - name: passwd
          mountPath: /etc/passwd
          readOnly: true
          subPath: passwd
        - name: group
          mountPath: /etc/group
          readOnly: true
          subPath: group
      type: none

  proxy:
    service:
      type: ClusterIP
    chp:
      networkPolicy:
        interNamespaceAccessLabels: accept
        # This currently causes Minikube deployment in GH-actions to fail.
        # We want it sometime but it's not critical; it will help with
        # scale-down
        # pdb:
        #   enabled: true
        #   minAvailable: 1

  # Any instantiation of this chart must also set ingress.hosts and add
  # the nginx.ingress.kubernetes.io/auth-signin annotation pointing to the
  # appropriate fully-qualified URLs for the Gafaelfawr /login route.
  ingress:
    enabled: true

    # -- Extra annotations to add to the ingress
    # @default -- See `values.yaml`
    annotations:
      nginx.ingress.kubernetes.io/auth-method: "GET"
      nginx.ingress.kubernetes.io/auth-response-headers: "Authorization,Cookie,X-Auth-Request-Email,X-Auth-Request-User,X-Auth-Request-Token"
      nginx.ingress.kubernetes.io/auth-url: "http://gafaelfawr.gafaelfawr.svc.cluster.local:8080/auth?scope=exec:notebook&notebook=true&minimum_lifetime=2160000"
      nginx.ingress.kubernetes.io/configuration-snippet: |
        auth_request_set $auth_www_authenticate $upstream_http_www_authenticate;
        auth_request_set $auth_status $upstream_http_x_error_status;
        auth_request_set $auth_error_body $upstream_http_x_error_body;
        error_page 403 = @autherror;
    ingressClassName: "nginx"
    pathSuffix: "*"

  cull:
    enabled: true
    timeout: 2592000  # 30 days -- shorten later
    every: 600  # Check every ten minutes
    users: true  # log out user when we cull
    removeNamedServers: true  # Post-stop hook may already do this
    maxAge: 5184000  # 60 days -- shorten later

  imagePullSecrets:
    - name: pull-secret

  scheduling:
    userScheduler:
      enabled: false
    userPlaceholder:
      enabled: false

config:
  # -- base_url must be set in each instantiation of this chart to the URL of
  # the primary ingress.  It's used to construct API requests to the
  # authentication service (which should go through the ingress).
  base_url: ""
  # -- butler_secret_path must be set here, because it's passed through to
  # the lab rather than being part of the Hub configuration.
  butler_secret_path: ""
  # -- pull_secret_path must also be set here; it specifies resources in
  # the lab namespace
  pull_secret_path: ""
  # -- images to pin to spawner menu
  pinned_images: []
  # -- Cachemachine image policy: "available" or "desired".  Use
  # "desired" at instances with streaming image support.
  cachemachine_image_policy: "available"
  # -- shut down user pods on logout.  Superfluous, because our
  # LogoutHandler enforces this in any event, but nice to make explicit.
  shutdown_on_logout: true
  # -- definitions of Lab sizes available in a given instance
  sizes:
    - name: Small
      cpu: 1
      ram: 3072M
    - name: Medium
      cpu: 2
      ram: 6144M
    - name: Large
      cpu: 4
      ram: 12288M
  # -- Volumes to use for a particular instance
  volumes: []
  # -- Where to mount volumes for a particular instance
  volume_mounts: []

  # -- Environment variables to set in spawned lab containers. Each value will
  # be expanded using Jinja 2 templating.
  # @default -- See `values.yaml`
  lab_environment:
    EXTERNAL_INSTANCE_URL: "{{ base_url }}"
    FIREFLY_ROUTE: /portal/app
    HUB_ROUTE: "{{ nublado_base_url }}"
    JS9_ROUTE: /js9
    API_ROUTE: /api
    TAP_ROUTE: /api/tap
    SODA_ROUTE: /api/image/soda
    WORKFLOW_ROUTE: /wf
    AUTO_REPO_URLS: https://github.com/lsst-sqre/notebook-demo
    NO_SUDO: "TRUE"
    EXTERNAL_GID: "{{ gid if gid else uid }}"
    EXTERNAL_GROUPS: "{{ external_groups }}"
    EXTERNAL_UID: "{{ uid }}"
    ACCESS_TOKEN: "{{ token }}"
    IMAGE_DIGEST: "{{ options.image_info.digest }}"
    IMAGE_DESCRIPTION: "{{ options.image_info.display_name }}"
    RESET_USER_ENV: "{{ options.reset_user_env }}"
    # We need to set CLEAR_DOTLOCAL until all images that didn't know
    # about RESET_USER_ENV have aged out (late 2022)
    CLEAR_DOTLOCAL: "{{ options.reset_user_env }}"
    DEBUG: "{{ options.debug }}"

  # -- Templates for the user resources to create for each lab spawn. This is
  # a string that can be templated and then loaded as YAML to generate a list
  # of Kubernetes objects to create.
  # @default -- See `values.yaml`
  user_resources_template: |
    - apiVersion: v1
      kind: Namespace
      metadata:
        name: "{{ user_namespace }}"
    - apiVersion: v1
      kind: ConfigMap
      metadata:
        name: group
        namespace: "{{ user_namespace }}"
      data:
        group: |
          root:x:0:
          bin:x:1:
          daemon:x:2:
          sys:x:3:
          adm:x:4:
          tty:x:5:
          disk:x:6:
          lp:x:7:
          mem:x:8:
          kmem:x:9:
          wheel:x:10:
          cdrom:x:11:
          mail:x:12:
          man:x:15:
          dialout:x:18:
          floppy:x:19:
          games:x:20:
          tape:x:33:
          video:x:39:
          ftp:x:50:
          lock:x:54:
          audio:x:63:
          nobody:x:99:
          users:x:100:
          utmp:x:22:
          utempter:x:35:
          input:x:999:
          systemd-journal:x:190:
          systemd-network:x:192:
          dbus:x:81:
          ssh_keys:x:998:
          lsst_lcl:x:1000:{{ user }}
          tss:x:59:
          cgred:x:997:
          screen:x:84:
          jovyan:x:768:{{ user }}{% for g in groups %}
          {{ g.name }}:x:{{ g.id }}:{{ user if g.id != gid else "" }}{% endfor %}
    - apiVersion: v1
      kind: ConfigMap
      metadata:
        name: passwd
        namespace: "{{ user_namespace }}"
      data:
        passwd: |
          root:x:0:0:root:/root:/bin/bash
          bin:x:1:1:bin:/bin:/sbin/nologin
          daemon:x:2:2:daemon:/sbin:/sbin/nologin
          adm:x:3:4:adm:/var/adm:/sbin/nologin
          lp:x:4:7:lp:/var/spool/lpd:/sbin/nologin
          sync:x:5:0:sync:/sbin:/bin/sync
          shutdown:x:6:0:shutdown:/sbin:/sbin/shutdown
          halt:x:7:0:halt:/sbin:/sbin/halt
          mail:x:8:12:mail:/var/spool/mail:/sbin/nologin
          operator:x:11:0:operator:/root:/sbin/nologin
          games:x:12:100:games:/usr/games:/sbin/nologin
          ftp:x:14:50:FTP User:/var/ftp:/sbin/nologin
          nobody:x:99:99:Nobody:/:/sbin/nologin
          systemd-network:x:192:192:systemd Network Management:/:/sbin/nologin
          dbus:x:81:81:System message bus:/:/sbin/nologin
          lsst_lcl:x:1000:1000::/home/lsst_lcl:/bin/bash
          tss:x:59:59:Account used by the trousers package to sandbox the tcsd daemon:/dev/null:/sbin/nologin
          {{ user }}:x:{{ uid }}:{{ gid if gid else uid }}::/home/{{ user }}:/bin/bash
    - apiVersion: v1
      kind: ConfigMap
      metadata:
        name: dask
        namespace: "{{ user_namespace }}"
      data:
        dask_worker.yml: |
          {{ dask_yaml | indent(6) }}
    # When we break out the resources we should make this per-instance
    #  configurable.
    - apiVersion: v1
      kind: ConfigMap
      metadata:
        name: idds-config
        namespace: "{{ user_namespace }}"
      data:
        idds_cfg.client.template: |
          # Licensed under the Apache License, Version 2.0 (the "License");
          # You may not use this file except in compliance with the License.
          # You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0
          #
          # Authors:
          # - Wen Guan, <wen.guan@cern.ch>, 2020
          [common]
          # if logdir is configured, idds will write to idds.log in this directory.
          # else idds will go to stdout/stderr.
          # With supervisord, it's good to write to stdout/stderr, then supervisord can manage and rotate logs.
          # logdir = /var/log/idds
          loglevel = INFO
          [rest]
          host = https://iddsserver.cern.ch:443/idds
          #url_prefix = /idds
          #cacher_dir = /tmp
          cacher_dir = /data/idds
    - apiVersion: v1
      kind: ServiceAccount
      metadata:
        name: "{{ user }}-serviceaccount"
        namespace: "{{ user_namespace }}"
      imagePullSecrets:
      - name: pull-secret
    - apiVersion: rbac.authorization.k8s.io/v1
      kind: Role
      metadata:
        name: "{{ user }}-role"
        namespace: "{{ user_namespace }}"
      rules:
      # cf https://kubernetes.dask.org/en/latest/kubecluster.html
        - apiGroups: [""]
          resources: ["pods", "services"]
          verbs: ["create", "delete", "get", "list", "watch"]
        - apiGroups: [""]
          resources: ["pods/log"]
          verbs: ["get","list"]
        - apiGroups: ["policy"]
          resources: ["poddisruptionbudgets"]
          verbs: ["create", "delete", "get", "list", "watch"]
    - apiVersion: rbac.authorization.k8s.io/v1
      kind: RoleBinding
      metadata:
        name: "{{ user }}-rolebinding"
        namespace: "{{ user_namespace }}"
      roleRef:
        apiGroup: rbac.authorization.k8s.io
        kind: Role
        name: "{{ user }}-role"
      subjects:
        - kind: ServiceAccount
          name: "{{ user }}-serviceaccount"
          namespace: "{{ user_namespace }}"
    - apiVersion: ricoberger.de/v1alpha1
      kind: VaultSecret
      metadata:
        name: butler-secret
        namespace: "{{ user_namespace }}"
      spec:
        path: "{{ butler_secret_path }}"
        type: Opaque
    - apiVersion: ricoberger.de/v1alpha1
      kind: VaultSecret
      metadata:
        name: pull-secret
        namespace: "{{ user_namespace }}"
      spec:
        path: "{{ pull_secret_path }}"
        type: kubernetes.io/dockerconfigjson

# Built-in network policy doesn't quite work (Labs can't talk to Hub,
# even with port 8081 explicitly enabled), so let's use our own for now.
network_policy:
  enabled: true

# The following will be set by parameters injected by Argo CD and should not
# be set in the individual environment values files.
global:
  # -- Base path for Vault secrets
  # @default -- Set by Argo CD
  vaultSecretsPath: ""
