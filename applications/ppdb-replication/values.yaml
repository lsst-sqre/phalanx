# Default values for ppdb-replication.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.

# This application will only ever be deployed to the USDF, so there are some
# values specific to that environment here.

# -- Number of deployment pods to start
replicaCount: 1

image:
  # -- Image to use in the ppdb-replication deployment
  repository: "ghcr.io/lsst/ppdb-replication"

  # -- Pull policy for the ppdb-replication image
  pullPolicy: "Always"

  # -- Tag of dax_ppdb image to use
  # @default -- The appVersion of the chart
  tag: "main"

ingress:
  # -- Additional annotations for the ingress rule
  annotations: {}

# -- Affinity rules for the ppdb-replication deployment pod
affinity: {}

# -- Node selection rules for the ppdb-replication deployment pod
nodeSelector: {}

# -- Annotations for the ppdb-replication deployment pod
podAnnotations: {}

# -- Resource limits and requests for the ppdb-replication deployment pod
# @default -- see `values.yaml`
resources:
  limits:
    cpu: "1"
    memory: "16.0Gi"
  requests:
    cpu: "200m"  # 20% of a single core
    memory: "4.0Gi"

# -- Tolerations for the ppdb-replication deployment pod
tolerations: []

# The following will be set by parameters injected by Argo CD and should not
# be set in the individual environment values files.
global:
  # -- Base URL for the environment
  # @default -- Set by Argo CD
  baseUrl: null

  # -- Host name for ingress
  # @default -- Set by Argo CD
  host: null

  # -- Base path for Vault secrets
  # @default -- Set by Argo CD
  vaultSecretsPath: null

# Application-specific configuration
config:
  # -- Logging level
  logLevel: "INFO"

  # -- Name of logger for monitoring
  monLogger: "lsst.dax.ppdb.monitor"

  # -- Logging profile (`production` for JSON, `development` for
  # human-friendly)
  logProfile: "production"

  # -- URL path prefix
  pathPrefix: "/ppdb-replication"

  # -- GCS bucket name
  gcsBucket: null

  # -- GCS bucket prefix
  gcsPrefix: null

  # -- APDB config file resource
  apdbConfig: null

  # -- PPDB config file resource
  ppdbConfig: null

  # -- Target BigQuery dataset
  dataset: null

  # -- Staging directory for replicated data
  stagingDirectory: null

  # -- Size of record batches when writing parquet files
  batchSize: 1000

  # -- APDB index URI
  apdbIndexUri: "/sdf/group/rubin/shared/apdb_config/apdb-index.yaml"

  # -- Comma-separated list of monitoring filter rules
  monRules: null

  # -- Allow updates to already replicated data
  updateExisting: false

  # -- Minimum time to wait before replicating a chunk after next chunk appears
  minWaitTime: 60

  # -- Maximum time to wait before replicating a chunk after next chunk appears
  maxWaitTime: 3600

  # -- Time to wait before checking for new chunks, if no chunk appears
  checkInterval: 30

  # -- Time to wait between uploader file scans
  waitInterval: 300

  # -- Time to wait between uploader file uploads
  uploadInterval: 0

  # -- Disable bucket validation in LSST S3 tools
  disableBucketValidation: 1

  # -- S3 endpoint URL
  s3EndpointUrl: https://s3dfrgw.slac.stanford.edu

  # -- S3 profile name for additional S3 profile
  additionalS3ProfileName: "embargo"

  # -- S3 profile URL for additional S3 profile
  additionalS3ProfileUrl: "https://sdfembs3.sdf.slac.stanford.edu"

  # -- Enable deletion of chunks after upload
  deleteChunks: false
