# Default values for qserv-kafka.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.

config:
  # -- Kafka consumer group ID
  consumerGroupId: "qserv"

  # -- Kafka topic for query cancellation requests
  jobCancelTopic: "lsst.tap.job-delete"

  # -- Kafka topic for query execution requests
  jobRunTopic: "lsst.tap.job-run"

  # -- Kafka topic for query status
  jobStatusTopic: "lsst.tap.job-status"

  # -- Logging level
  logLevel: "INFO"

  # -- Logging profile (`production` for JSON, `development` for
  # human-friendly)
  logProfile: "production"

  # -- Maximum number of arq jobs each worker can process simultaneously
  maxWorkerJobs: 2

  metrics:
    # -- Whether to enable sending metrics
    enabled: false

    # -- Name under which to log metrics. Generally there is no reason to
    # change this.
    application: "qservkafka"

    events:
      # -- Topic prefix for events. It may sometimes be useful to change this
      # in development environments.
      topicPrefix: "lsst.square.metrics.events"

    schemaManager:
      # -- URL of the Confluent-compatible schema registry server
      # @default -- Sasquatch in the local cluster
      registryUrl: "http://sasquatch-schema-registry.sasquatch.svc.cluster.local:8081"

      # -- Suffix to add to all registered subjects. This is sometimes useful
      # for experimentation during development.
      suffix: ""

  # -- Extra database connections that may be opened in excess of the pool
  # size to handle surges in load. This is used primarily by the frontend for
  # jobs that complete immediately.
  qservDatabaseOverflow: 50

  # -- Database pool size. This is the number of MySQL connections that will
  # be held open regardless of load. This should generally be set to the same
  # as `maxWorkerJobs`.
  qservDatabasePoolSize: 2

  # -- URL to the Qserv MySQL interface (must use a scheme of `mysql+asyncmy`)
  # @default -- None, must be set
  qservDatabaseUrl: null

  # -- Interval at which Qserv is polled for query status in Safir
  # `parse_timedelta` format
  qservPollInterval: "1s"

  # -- Maximum simultaneous connections to open to the REST API.
  qservRestMaxConnections: 20

  # -- Timeout for REST API calls in Safir `parse_timedelta` format. This
  # includes time spent waiting for a connection if the maximum number of
  # connections has been reached.
  qservRestTimeout: "30s"

  # -- URL to the Qserv REST API
  # @default -- None, must be set
  qservRestUrl: null

  # -- How long to wait for result processing (retrieval and upload) before
  # timing out, in seconds. This doubles as the timeout forcibly terminating
  # result worker pods.
  # @default -- 3600 (1 hour)
  resultTimeout: 3600

image:
  # -- Image to use in the qserv-kafka deployment
  repository: "ghcr.io/lsst-sqre/qserv-kafka"

  # -- Pull policy for the qserv-kafka image
  pullPolicy: "IfNotPresent"

  # -- Tag of image to use
  # @default -- The appVersion of the chart
  tag: null

ingress:
  # -- Additional annotations for the ingress rule
  annotations: {}

frontend:
  # -- Affinity rules for the qserv-kafka frontend pod
  affinity: {}

  # -- Whether to allow containers to run as root. Set to true to allow use of
  # debug containers to diagnose issues such as memory leaks.
  allowRootDebug: false

  # -- Node selection rules for the qserv-kafka frontend pod
  nodeSelector: {}

  # -- Annotations for the qserv-kafka frontend pod
  podAnnotations: {}

  # -- Resource limits and requests for the qserv-kafka frontend pod
  # @default -- See `values.yaml`
  resources:
    limits:
      cpu: "1"
      memory: "300Mi"
    requests:
      cpu: "100m"
      memory: "145Mi"

  # -- Tolerations for the qserv-kafka frontend pod
  tolerations: []

redis:
  config:
    # -- Name of secret containing Redis password
    secretName: "qserv-kafka"

    # -- Key inside secret from which to get the Redis password (do not
    # change)
    secretKey: "redis-password"

  persistence:
    # -- Whether to persist Redis storage. Setting this to false will use
    # `emptyDir` and lose track of all queries on restart. Only use this for a
    # test deployment.
    enabled: true

    # -- Access mode of storage to request
    accessMode: "ReadWriteOnce"

    # -- Amount of persistent storage to request
    size: "100Mi"

    # -- Class of storage to request
    storageClass: null

    # -- Use an existing PVC, not dynamic provisioning. If this is set, the
    # size, storageClass, and accessMode settings are ignored.
    volumeClaimName: null

  # -- Resource limits and requests for the Redis pod
  # @default -- See `values.yaml`
  resources:
    limits:
      cpu: "1"
      memory: "100Mi"
    requests:
      cpu: "10m"
      memory: "20Mi"

resultWorker:
  # -- Affinity rules for the qserv-kafka worker pods
  affinity: {}

  # -- Whether to allow containers to run as root. Set to true to allow use of
  # debug containers to diagnose issues such as memory leaks.
  allowRootDebug: false

  autoscaling:
    # -- Enable autoscaling of qserv-kafka result workers
    enabled: true

    # -- Minimum number of qserv-kafka worker pods
    minReplicas: 1

    # -- Maximum number of qserv-kafka worker pods. Each replica will open
    # database connections up to the configured pool size and overflow limits,
    # so make sure the combined connections are under the postgres connection
    # limit.
    maxReplicas: 10

    # -- Target CPU utilization of qserv-kafka worker pods.
    targetCPUUtilizationPercentage: 75

  # -- Node selection rules for the qserv-kafka worker pods
  nodeSelector: {}

  # -- Annotations for the qserv-kafka worker pods
  podAnnotations: {}

  # -- Number of result worker pods to start if autoscaling is disabled
  replicaCount: 1

  # -- Resource limits and requests for the qserv-kafka worker pods
  # @default -- See `values.yaml`
  resources:
    limits:
      cpu: "1"
      memory: "300Mi"
    requests:
      cpu: "1"
      memory: "145Mi"

  # -- Tolerations for the qserv-kafka worker pods
  tolerations: []

# The following will be set by parameters injected by Argo CD and should not
# be set in the individual environment values files.
global:
  # -- Base URL for the environment
  # @default -- Set by Argo CD
  baseUrl: null

  # -- Host name for ingress
  # @default -- Set by Argo CD
  host: null

  # -- Base path for Vault secrets
  # @default -- Set by Argo CD
  vaultSecretsPath: null
