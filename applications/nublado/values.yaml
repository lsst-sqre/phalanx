# Default values for Nublado.

controller:
  image:
    # -- nublado image to use
    repository: ghcr.io/lsst-sqre/jupyterlab-controller

    # -- Pull policy for the nublado image
    pullPolicy: IfNotPresent

    # -- Tag of nublado image to use
    # @default -- The appVersion of the chart
    tag: ""

  # -- Affinity rules for the lab controller pod
  affinity: {}

  # -- Node selector rules for the lab controller pod
  nodeSelector: {}

  # -- Annotations for the lab controller pod
  podAnnotations: {}

  # -- Resource limits and requests for the lab controller pod
  resources: {}

  # -- Tolerations for the lab controller pod
  tolerations: []

  ingress:
    # -- Additional annotations to add for the lab controller pod ingress
    annotations: {}

  # -- If Google Artifact Registry is used as the image source, the Google
  # service account that has an IAM binding to the `nublado-controller`
  # Kubernetes service account and has the Artifact Registry reader role
  # @default -- None, must be set when using Google Artifact Registry
  googleServiceAccount: ""

  # -- Whether to enable Slack alerts. If set to true, `slack_webhook` must be
  # set in the corresponding Nublado Vault secret.
  slackAlerts: false


  # Passed as YAML to the lab controller.
  config:
    images:
      # -- Source for prepulled images. For Docker, set `type` to `docker`,
      # `registry` to the hostname and `repository` to the name of the
      # repository. For Google Artifact Repository, set `type` to `google`,
      # `location` to the region, `projectId` to the Google project,
      # `repository` to the name of the repository, and `image` to the name of
      # the image.
      # @default -- None, must be specified
      source: {}

      # -- Tag marking the recommended image (shown first in the menu)
      recommendedTag: "recommended"

      # -- Number of most-recent releases to prepull.
      numReleases: 1

      # -- Number of most-recent weeklies to prepull.
      numWeeklies: 2

      # -- Number of most-recent dailies to prepull.
      numDailies: 3

      # -- Restrict images to this SAL cycle, if given.
      cycle: null

      # -- List of additional image tags to prepull. Listing the image tagged
      # as recommended here is recommended when using a Docker image source to
      # ensure its name can be expanded properly in the menu.
      pin: []

      # -- Additional tags besides `recommendedTag` that should be recognized
      # as aliases.
      aliasTags: []

    lab:
      # -- Environment variables to set for every user lab.
      # @default -- See `values.yaml`
      env:
        API_ROUTE: "/api"
        AUTO_REPO_SPECS: "https://github.com/lsst-sqre/system-test@prod"
        CULL_KERNEL_IDLE_TIMEOUT: "432000"  # These might be set from group?
        CULL_KERNEL_CONNECTED: "True"
        CULL_KERNEL_INTERVAL: "300"
        FIREFLY_ROUTE: "/portal/app"
        HUB_ROUTE: "/nb/hub"
        NO_ACTIVITY_TIMEOUT: "432000"  # Also from group?
        TAP_ROUTE: "/api/tap"

      # -- Containers run as init containers with each user pod. Each should
      # set `name`, `image` (a Docker image reference), and `privileged`, and
      # may contain `volumes` (similar to the main `volumes`
      # configuration). If `privileged` is true, the container will run as
      # root with `allowPrivilegeEscalation` true. Otherwise it will, run as
      # UID 1000.
      initcontainers: []

      # -- Pull secret to use for labs. Set to the string `pull-secret` to use
      # the normal pull secret from Vault.
      # @default -- Do not use a pull secret
      pullSecret: null

      # -- Secrets to set in the user pods. Each should have a `secretKey` key
      # pointing to a secret in the same namespace as the controller
      # (generally `nublado-secret`) and `secretRef` pointing to a field in
      # that key.
      secrets: []

      # -- Available lab sizes. Names must be chosen from `fine`,
      # `diminutive`, `tiny`, `small`, `medium`, `large`, `huge`,
      # `gargantuan`, and `colossal` in that order. Each should specify the
      # maximum CPU equivalents and memory. SI prefixes for memory are
      # supported.
      # @default -- See `values.yaml` (specifies `small`, `medium`, and
      # `large`)
      sizes:
        small:
          cpu: 1.0
          memory: 3Gi
        medium:
          cpu: 2.0
          memory: 6Gi
        large:
          cpu: 4.0
          memory: 12Gi

      # -- Volumes that should be mounted in lab pods. Currently this only
      # supports NFS volumes and must specify `containerPath`, `server`,
      # `serverPath`, and `mode` where mode is one of `ro` or `rw`.
      volumes: {}

      # -- Files to be mounted as ConfigMaps inside the user lab pod.
      # `contents` contains the file contents. Set `modify` to true to make
      # the file writable in the pod.
      # @default -- See `values.yaml`
      files:
        /etc/passwd:
          modify: true
          contents: |
            root:x:0:0:root:/root:/bin/bash
            bin:x:1:1:bin:/bin:/sbin/nologin
            daemon:x:2:2:daemon:/sbin:/sbin/nologin
            adm:x:3:4:adm:/var/adm:/sbin/nologin
            lp:x:4:7:lp:/var/spool/lpd:/sbin/nologin
            sync:x:5:0:sync:/sbin:/bin/sync
            shutdown:x:6:0:shutdown:/sbin:/sbin/shutdown
            halt:x:7:0:halt:/sbin:/sbin/halt
            mail:x:8:12:mail:/var/spool/mail:/sbin/nologin
            operator:x:11:0:operator:/root:/sbin/nologin
            games:x:12:100:games:/usr/games:/sbin/nologin
            ftp:x:14:50:FTP User:/var/ftp:/sbin/nologin
            tss:x:59:59:Account used by the trousers package to sandbox the tcsd daemon:/dev/null:/sbin/nologin
            dbus:x:81:81:System message bus:/:/sbin/nologin
            nobody:x:99:99:Nobody:/:/sbin/nologin
            systemd-network:x:192:192:systemd Network Management:/:/sbin/nologin
            lsst_lcl:x:1000:1000::/home/lsst_lcl:/bin/bash
        /etc/group:
          modify: true
          contents: |
            root:x:0:
            bin:x:1:
            daemon:x:2:
            sys:x:3:
            adm:x:4:
            tty:x:5:
            disk:x:6:
            lp:x:7:
            mem:x:8:
            kmem:x:9:
            wheel:x:10:
            cdrom:x:11:
            mail:x:12:
            man:x:15:
            dialout:x:18:
            floppy:x:19:
            games:x:20:
            utmp:x:22:
            tape:x:33:
            utempter:x:35:
            video:x:39:
            ftp:x:50:
            lock:x:54:
            tss:x:59:
            audio:x:63:
            dbus:x:81:
            screen:x:84:
            nobody:x:99:
            users:x:100:
            systemd-journal:x:190:
            systemd-network:x:192:
            cgred:x:997:
            ssh_keys:x:998:
            input:x:999:
        /opt/lsst/software/jupyterlab/lsst_dask.yml:
          modify: false
          contents: |
            # No longer used, but preserves compatibility with runlab.sh
            dask_worker.yml: |
              enabled: false
        /opt/lsst/software/jupyterlab/panda:
          modify: false
          contents: |
            # Licensed under the Apache License, Version 2.0 (the "License");
            # You may not use this file except in compliance with the License.
            # You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0
            #
            # Authors:
            # - Wen Guan, <wen.guan@cern.ch>, 2020
            [common]
            # if logdir is configured, idds will write to idds.log in this directory.
            # else idds will go to stdout/stderr.
            # With supervisord, it's good to write to stdout/stderr, then supervisord can manage and rotate logs.
            # logdir = /var/log/idds
            loglevel = INFO
            [rest]
            host = https://iddsserver.cern.ch:443/idds
            #url_prefix = /idds
            #cacher_dir = /tmp
            cacher_dir = /data/idds

    safir:
      # -- Level of Python logging
      logLevel: "INFO"

      # -- Path prefix that will be routed to the controller
      pathPrefix: "/nublado"

  # The controller also manages file serving environments
  fileserver:
    # -- Image for fileserver container
    # @default -- None, must be specified
    image: ghcr.io/lsst-sqre/worblehat

    # -- Tag for fileserver container
    # @default -- `latest`
    tag: 0.1.0

    # -- Pull policy for fileserver container
    # @default -- `IfNotPresent`
    pullPolicy: IfNotPresent

    # -- Timeout for user fileservers, in seconds
    # @default -- 3600
    timeout: 64800

    # -- Namespace for user fileservers
    # @default -- `fileservers`
    namespace: fileservers


# JupyterHub configuration handled directly by this chart rather than by Zero
# to JupyterHub.
hub:
  timeout:
    # -- Timeout for the Kubernetes spawn process in seconds. (Allow long
    # enough to pull uncached images if needed.)
    spawn: 600

    # -- Timeout for JupyterLab to start. Currently this sometimes takes over
    # 60 seconds for reasons we don't understand.
    startup: 90

# Configuration for the Zero to JupyterHub subchart.
jupyterhub:
  hub:
    authenticatePrometheus: false
    image:
      name: ghcr.io/lsst-sqre/rsp-restspawner
      tag: 0.2.0
    resources:
      limits:
        cpu: 900m
        memory: 1Gi  # Should support about 200 users
    db:
      # Password comes from the nublado-secret.
      type: "postgres"
      password: "true"
      url: "postgresql://jovyan@postgres.postgres/jupyterhub"
    containerSecurityContext:
      runAsUser: 768
      runAsGroup: 768
      allowPrivilegeEscalation: false
    baseUrl: "/nb"
    existingSecret: "nublado-secret"
    # This will need work
    extraEnv:
      JUPYTERHUB_CRYPT_KEY:
        valueFrom:
          secretKeyRef:
            name: "nublado-secret"
            key: "hub.config.CryptKeeper.keys"
    extraVolumes:
      - name: "hub-config"
        configMap:
          name: "hub-config"
      - name: "hub-gafaelfawr-token"
        secret:
          secretName: "hub-gafaelfawr-token"
    extraVolumeMounts:
      - name: "hub-config"
        mountPath: "/usr/local/etc/jupyterhub/jupyterhub_config.d"
      - name: "hub-gafaelfawr-token"
        mountPath: "/etc/gafaelfawr"
    # We still have to use our own, enabled at the top level, which is
    #  similar but not identical.  This one still doesn't work, even if
    #  you explicitly enable port 8081 so the labs can talk to the Hub.
    networkPolicy:
      enabled: false
    loadRoles:
      self:
        scopes: ['admin:servers!user', 'read:metrics']
      server:
        scopes: ['inherit']  # Let server use API like user

  prePuller:
    continuous:
      enabled: false
    hook:
      enabled: false

  singleuser:
    cloudMetadata:
      blockWithIptables: false
    cmd: "/opt/lsst/software/jupyterlab/runlab.sh"
    defaultUrl: "/lab"

  proxy:
    service:
      type: ClusterIP
    # Replace with traefik
    chp:
      networkPolicy:
        interNamespaceAccessLabels: accept
        # This currently causes Minikube deployment in GH-actions to fail.
        # We want it sometime but it's not critical; it will help with
        # scale-down
        # pdb:
        #   enabled: true
        #   minAvailable: 1

  # Rather than using the JupyterHub-provided ingress, which requires us
  # to repeat the global host name, we just use a GafaelfawrIngress of
  # our own, found in templates.  That also means the annotations are
  # automatically in sync.
  ingress:
    enabled: false

  cull:
    enabled: true
    timeout: 2592000  # 30 days -- shorten later
    every: 600  # Check every ten minutes
    users: true  # log out user when we cull
    removeNamedServers: true  # Post-stop hook may already do this
    maxAge: 5184000  # 60 days -- shorten later

  scheduling:
    userScheduler:
      enabled: false
    userPlaceholder:
      enabled: false

# The following will be set by parameters injected by Argo CD and should not
# be set in the individual environment values files.
global:
  # -- Base URL for the environment
  # @default -- Set by Argo CD
  baseUrl: ""

  # -- Host name for ingress
  # @default -- Set by Argo CD
  host: ""

  # -- Base path for Vault secrets
  # @default -- Set by Argo CD
  vaultSecretsPath: ""
