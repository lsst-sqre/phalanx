apiVersion: apps/v1
kind: Deployment
metadata:
  name: "nublado-controller"
  labels:
    {{- include "nublado.labels" . | nindent 4 }}
  annotations:
    # If the controller is unavailable when the hub comes up, the hub will
    # think all of the labs are unresponsive and shut them them down. This puts
    # the controller deployment in a later ArgoCD sync wave than the hub, which
    # means it won't get synced until after the hub is synced:
    #
    # https://argo-cd.readthedocs.io/en/stable/user-guide/sync-waves/#how-do-i-configure-waves
    # https://phalanx.lsst.io/applications/nublado/upgrade.html#routine-upgrades
    argocd.argoproj.io/sync-wave: "1"
spec:
  replicas: 1
  selector:
    matchLabels:
      {{- include "nublado.selectorLabels" . | nindent 6 }}
  strategy:
    type: "Recreate"
  template:
    metadata:
      annotations:
        checksum/config: {{ include (print $.Template.BasePath "/controller-configmap.yaml") . | sha256sum }}
        {{- with .Values.controller.podAnnotations }}
        {{- toYaml . | nindent 8 }}
        {{- end }}
      labels:
        {{- include "nublado.selectorLabels" . | nindent 8 }}
    spec:
      {{- with .Values.controller.affinity }}
      affinity:
        {{- toYaml . | nindent 8 }}
      {{- end }}
      containers:
        - name: "{{ .Chart.Name }}-controller"
          command:
            - "uvicorn"
            - "nublado.controller.main:create_app"
            - "--port"
            - "8080"
            - "--host"
            - "0.0.0.0"
          env:
            - name: EXTERNAL_INSTANCE_URL
              value: {{ .Values.global.baseUrl | quote }}
            {{- if .Values.controller.config.metrics.enabled }}
            - name: "KAFKA_BOOTSTRAP_SERVERS"
              valueFrom:
                secretKeyRef:
                  name: "nublado-kafka"
                  key: "bootstrapServers"
            - name: "KAFKA_SECURITY_PROTOCOL"
              valueFrom:
                secretKeyRef:
                  name: "nublado-kafka"
                  key: "securityProtocol"
            - name: "KAFKA_CLIENT_CERT_PATH"
              value: "/etc/nublado-kafka/user.crt"
            - name: "KAFKA_CLIENT_KEY_PATH"
              value: "/etc/nublado-kafka/user.key"
            - name: "KAFKA_CLUSTER_CA_PATH"
              value: "/etc/nublado-kafka/ca.crt"
            {{- end }}
            {{- if .Values.controller.slackAlerts }}
            - name: "NUBLADO_SLACK_WEBHOOK"
              valueFrom:
                secretKeyRef:
                  name: "nublado-secret"
                  key: "slack-webhook"
            {{- end }}
            - name: "REPERTOIRE_BASE_URL"
              value: {{ .Values.global.repertoireUrl | quote }}
            {{- if .Values.sentry.enabled }}
            - name: "SENTRY_DSN"
              valueFrom:
                secretKeyRef:
                  name: "nublado-secret"
                  key: "sentry-dsn"
            - name: "SENTRY_ENVIRONMENT"
              value: {{ .Values.global.environmentName }}
            {{- end }}
            # Uvicorn keepalive timeout, in seconds. This should be longer
            # than any keepalive timeouts on any downstream proxies. The
            # keepalive timeout for ingress-nginx connections is 60s.
            - name: "UVICORN_TIMEOUT_KEEP_ALIVE"
              value: "61"
          image: "{{ .Values.image.repository }}:{{ .Values.image.tag | default .Chart.AppVersion }}"
          imagePullPolicy: {{ .Values.image.pullPolicy | quote }}
          lifecycle:
            # Number of seconds k8s should wait before sending SIGTERM on
            # graceful restarts/deployments/shutdowns. We need this because it
            # takes ingress-nginx some time to configure itself to stop
            # sending traffic to the pods scheduled for termination. If
            # Uvicorn gets the SIGTERM and starts closing connections, we
            # could end up with a TCP race condition if ingress-nginx still
            # tries to send data over those connections.
            #
            # Note that terminationGracePeriodSeconds includes the time that
            # is spent in the preStop hook. If you’re adding a preStop hook
            # and your terminationGracePeriodSeconds is super fine-tuned, then
            # you should update your terminationGracePeriodSeconds to add the
            # amount of time you’ll be spending in your preStop hook.
            preStop:
              sleep:
                seconds: 10
          ports:
            - name: "http"
              containerPort: 8080
              protocol: "TCP"
          readinessProbe:
            httpGet:
              path: "/"
              port: "http"
          {{- with .Values.controller.resources }}
          resources:
            {{- toYaml . | nindent 12 }}
          {{- end }}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
                - "all"
            readOnlyRootFilesystem: true
          startupProbe:
            httpGet:
              path: "/"
              port: "http"
            failureThreshold: 30
            periodSeconds: 10
          volumeMounts:
            - name: "config"
              mountPath: "/etc/nublado"
              readOnly: true
            {{- if eq "docker" .Values.controller.config.images.source.type }}
            - name: "docker-creds"
              mountPath: "/etc/secrets"
              readOnly: true
            {{- end }}
            {{- if .Values.controller.config.metrics.enabled }}
            - name: "kafka"
              mountPath: "/etc/nublado-kafka/ca.crt"
              readOnly: true
              subPath: "ssl.truststore.crt"
            - name: "kafka"
              mountPath: "/etc/nublado-kafka/user.crt"
              readOnly: true
              subPath: "ssl.keystore.crt"
            - name: "kafka"
              mountPath: "/etc/nublado-kafka/user.key"
              readOnly: true
              subPath: "ssl.keystore.key"
            {{- end }}
            - name: "podinfo"
              mountPath: "/etc/podinfo"
      {{- with .Values.controller.nodeSelector }}
      nodeSelector:
        {{- toYaml . | nindent 8 }}
      {{- end }}
      serviceAccountName: "nublado-controller"
      securityContext:
        runAsNonRoot: true
        runAsUser: 1000
        runAsGroup: 1000
      {{- with .Values.controller.tolerations }}
      tolerations:
        {{- toYaml . | nindent 8 }}
      {{- end }}
      volumes:
        - name: "config"
          configMap:
            name: "nublado-controller-config"
        {{- if eq "docker" .Values.controller.config.images.source.type }}
        - name: "docker-creds"
          secret:
            secretName: "pull-secret"
        {{- end }}
        {{- if .Values.controller.config.metrics.enabled }}
        - name: "kafka"
          secret:
            secretName: "nublado-kafka"
        {{- end }}
        - name: "podinfo"
          downwardAPI:
            items:
              - path: "name"
                fieldRef:
                  fieldPath: "metadata.name"
              - path: "namespace"
                fieldRef:
                  fieldPath: "metadata.namespace"
              - path: "uid"
                fieldRef:
                  fieldPath: "metadata.uid"
