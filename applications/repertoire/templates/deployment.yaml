apiVersion: apps/v1
kind: Deployment
metadata:
  name: "repertoire"
  labels:
    {{- include "repertoire.labels" . | nindent 4 }}
spec:
  replicas: {{ .Values.replicaCount }}
  selector:
    matchLabels:
      {{- include "repertoire.selectorLabels" . | nindent 6 }}
  template:
    metadata:
      annotations:
        checksum/config: {{ include (print $.Template.BasePath "/configmap.yaml") . | sha256sum }}
        {{- with .Values.podAnnotations }}
        {{- toYaml . | nindent 8 }}
        {{- end }}
      labels:
        {{- include "repertoire.selectorLabels" . | nindent 8 }}
    spec:
      {{- with .Values.affinity }}
      affinity:
        {{- toYaml . | nindent 8 }}
      {{- end }}
      automountServiceAccountToken: false
      containers:
        - name: {{ .Chart.Name }}
          {{- if .Values.config.slackAlerts }}
          env:
            - name: "REPERTOIRE_SLACK_WEBHOOK"
              valueFrom:
                secretKeyRef:
                  name: "repertoire"
                  key: "slack-webhook"
            {{- if .Values.config.hips.datasets }}
            - name: "REPERTOIRE_TOKEN"
              valueFrom:
                secretKeyRef:
                  name: "repertoire-token"
                  key: "token"
            {{- end }}
            # Uvicorn keepalive timeout, in seconds. This should be longer
            # than any keepalive timeouts on any downstream proxies. The
            # keepalive timeout for ingress-nginx connections is 60s.
            - name: "UVICORN_TIMEOUT_KEEP_ALIVE"
              value: "61"
          {{- end }}
          envFrom:
            - configMapRef:
                name: "repertoire"
          image: "{{ .Values.image.repository }}:{{ .Values.image.tag | default .Chart.AppVersion }}"
          imagePullPolicy: {{ .Values.image.pullPolicy }}
          lifecycle:
            # Number of seconds k8s should wait before sending SIGTERM on
            # graceful restarts/deployments/shutdowns. We need this because it
            # takes ingress-nginx some time to configure itself to stop
            # sending traffic to the pods scheduled for termination. If
            # Uvicorn gets the SIGTERM and starts closing connections, we
            # could end up with a TCP race condition if ingress-nginx still
            # tries to send data over those connections.
            #
            # Note that terminationGracePeriodSeconds includes the time that
            # is spent in the preStop hook. If you’re adding a preStop hook
            # and your terminationGracePeriodSeconds is super fine-tuned, then
            # you should update your terminationGracePeriodSeconds to add the
            # amount of time you’ll be spending in your preStop hook.
            preStop:
              sleep:
                seconds: 10
          ports:
            - name: "http"
              containerPort: 8080
              protocol: "TCP"
          readinessProbe:
            httpGet:
              path: "/"
              port: "http"
          resources:
            {{- toYaml .Values.resources | nindent 12 }}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
                - "all"
            readOnlyRootFilesystem: true
          volumeMounts:
            - name: "config"
              mountPath: "/etc/repertoire"
              readOnly: true
            - name: "secrets"
              mountPath: "/etc/repertoire/secrets"
              readOnly: true
      {{- with .Values.nodeSelector }}
      nodeSelector:
        {{- toYaml . | nindent 8 }}
      {{- end }}
      {{- with .Values.tolerations }}
      tolerations:
        {{- toYaml . | nindent 8 }}
      {{- end }}
      securityContext:
        runAsNonRoot: true
        runAsUser: 1000
        runAsGroup: 1000
      volumes:
        - name: "config"
          configMap:
            name: "repertoire"
        - name: "secrets"
          secret:
            secretName: "repertoire"
