# Default values for Nublado.

controller:
  image:
    # -- nublado image to use
    repository: ghcr.io/lsst-sqre/jupyterlab-controller

    # -- Pull policy for the nublado image
    pullPolicy: IfNotPresent

    # -- Tag of nublado image to use
    # @default -- The appVersion of the chart
    tag: ""

  # -- Affinity rules for the lab controller pod
  affinity: {}

  # -- Node selector rules for the lab controller pod
  nodeSelector: {}

  # -- Annotations for the lab controller pod
  podAnnotations: {}

  # -- Resource limits and requests for the lab controller pod
  resources: {}

  # -- Tolerations for the lab controller pod
  tolerations: []

  ingress:
    # -- Additional annotations to add for the lab controller pod ingress
    annotations: {}

  # -- If Google Artifact Registry is used as the image source, the Google
  # service account that has an IAM binding to the `nublado-controller`
  # Kubernetes service account and has the Artifact Registry reader role
  # @default -- None, must be set when using Google Artifact Registry
  googleServiceAccount: ""

  # -- Whether to enable Slack alerts. If set to true, `slack_webhook` must be
  # set in the corresponding Nublado Vault secret.
  slackAlerts: false

  # Passed as YAML to the lab controller.
  config:
    fileserver:
      # -- Enable fileserver management
      enabled: false

      # -- Image for fileserver container
      image: ghcr.io/lsst-sqre/worblehat

      # -- Tag for fileserver container
      tag: 0.1.0

      # -- Pull policy for fileserver container
      pullPolicy: IfNotPresent

      # -- Timeout for user fileservers, in seconds
      timeout: 3600

      # -- Namespace for user fileservers
      namespace: fileservers

    images:
      # -- Source for prepulled images. For Docker, set `type` to `docker`,
      # `registry` to the hostname and `repository` to the name of the
      # repository. For Google Artifact Repository, set `type` to `google`,
      # `location` to the region, `projectId` to the Google project,
      # `repository` to the name of the repository, and `image` to the name of
      # the image.
      # @default -- None, must be specified
      source: {}

      # -- Tag marking the recommended image (shown first in the menu)
      recommendedTag: "recommended"

      # -- Number of most-recent releases to prepull.
      numReleases: 1

      # -- Number of most-recent weeklies to prepull.
      numWeeklies: 2

      # -- Number of most-recent dailies to prepull.
      numDailies: 3

      # -- Restrict images to this SAL cycle, if given.
      cycle: null

      # -- List of additional image tags to prepull. Listing the image tagged
      # as recommended here is recommended when using a Docker image source to
      # ensure its name can be expanded properly in the menu.
      pin: []

      # -- Additional tags besides `recommendedTag` that should be recognized
      # as aliases.
      aliasTags: []

    lab:
      # -- Environment variables to set for every user lab.
      # @default -- See `values.yaml`
      env:
        API_ROUTE: "/api"
        AUTO_REPO_SPECS: "https://github.com/lsst-sqre/system-test@prod"
        CULL_KERNEL_IDLE_TIMEOUT: "432000"  # These might be set from group?
        CULL_KERNEL_CONNECTED: "True"
        CULL_KERNEL_INTERVAL: "300"
        FIREFLY_ROUTE: "/portal/app"
        HUB_ROUTE: "/nb/hub"
        NO_ACTIVITY_TIMEOUT: "432000"  # Also from group?
        TAP_ROUTE: "/api/tap"

      # -- Containers run as init containers with each user pod. Each should
      # set `name`, `image` (a Docker image reference), and `privileged`, and
      # may contain `volumes` (similar to the main `volumes`
      # configuration). If `privileged` is true, the container will run as
      # root with `allowPrivilegeEscalation` true. Otherwise it will, run as
      # UID 1000.
      initcontainers: []

      # -- Pull secret to use for labs. Set to the string `pull-secret` to use
      # the normal pull secret from Vault.
      # @default -- Do not use a pull secret
      pullSecret: null

      # -- Secrets to set in the user pods. Each should have a `secretKey` key
      # pointing to a secret in the same namespace as the controller
      # (generally `nublado-secret`) and `secretRef` pointing to a field in
      # that key.
      secrets: []

      # -- Available lab sizes. Names must be chosen from `fine`,
      # `diminutive`, `tiny`, `small`, `medium`, `large`, `huge`,
      # `gargantuan`, and `colossal` in that order. Each should specify the
      # maximum CPU equivalents and memory. SI prefixes for memory are
      # supported.
      # @default -- See `values.yaml` (specifies `small`, `medium`, and
      # `large`)
      sizes:
        small:
          cpu: 1.0
          memory: 4Gi
        medium:
          cpu: 2.0
          memory: 8Gi
        large:
          cpu: 4.0
          memory: 16Gi

      # -- Volumes that should be mounted in lab pods. This supports NFS,
      # HostPath, and PVC volume types (differentiated in source.type)
      volumes: []
      # volumes:
      # - containerPath: "/project"
      #   mode: "rw"
      #   source:
      #     type: nfs
      #     serverPath: "/share1/project"
      #     server: "10.87.86.26"

      # -- Files to be mounted as ConfigMaps inside the user lab pod.
      # `contents` contains the file contents. Set `modify` to true to make
      # the file writable in the pod.
      # @default -- See `values.yaml`
      files:
        /etc/passwd:
          modify: true
          contents: |
            root:x:0:0:root:/root:/bin/bash
            bin:x:1:1:bin:/bin:/sbin/nologin
            daemon:x:2:2:daemon:/sbin:/sbin/nologin
            adm:x:3:4:adm:/var/adm:/sbin/nologin
            lp:x:4:7:lp:/var/spool/lpd:/sbin/nologin
            sync:x:5:0:sync:/sbin:/bin/sync
            shutdown:x:6:0:shutdown:/sbin:/sbin/shutdown
            halt:x:7:0:halt:/sbin:/sbin/halt
            mail:x:8:12:mail:/var/spool/mail:/sbin/nologin
            operator:x:11:0:operator:/root:/sbin/nologin
            games:x:12:100:games:/usr/games:/sbin/nologin
            ftp:x:14:50:FTP User:/var/ftp:/sbin/nologin
            tss:x:59:59:Account used by the trousers package to sandbox the tcsd daemon:/dev/null:/sbin/nologin
            dbus:x:81:81:System message bus:/:/sbin/nologin
            nobody:x:99:99:Nobody:/:/sbin/nologin
            systemd-network:x:192:192:systemd Network Management:/:/sbin/nologin
            lsst_lcl:x:1000:1000::/home/lsst_lcl:/bin/bash
        /etc/group:
          modify: true
          contents: |
            root:x:0:
            bin:x:1:
            daemon:x:2:
            sys:x:3:
            adm:x:4:
            tty:x:5:
            disk:x:6:
            lp:x:7:
            mem:x:8:
            kmem:x:9:
            wheel:x:10:
            cdrom:x:11:
            mail:x:12:
            man:x:15:
            dialout:x:18:
            floppy:x:19:
            games:x:20:
            utmp:x:22:
            tape:x:33:
            utempter:x:35:
            video:x:39:
            ftp:x:50:
            lock:x:54:
            tss:x:59:
            audio:x:63:
            dbus:x:81:
            screen:x:84:
            nobody:x:99:
            users:x:100:
            systemd-journal:x:190:
            systemd-network:x:192:
            cgred:x:997:
            ssh_keys:x:998:
            input:x:999:
        /opt/lsst/software/jupyterlab/lsst_dask.yml:
          modify: false
          contents: |
            # No longer used, but preserves compatibility with runlab.sh
            dask_worker.yml: |
              enabled: false
        /opt/lsst/software/jupyterlab/panda/idds.cfg.client.template:
          modify: false
          contents: |
            # Licensed under the Apache License, Version 2.0 (the "License");
            # You may not use this file except in compliance with the License.
            # You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0
            #
            # Authors:
            # - Wen Guan, <wen.guan@cern.ch>, 2020
            [common]
            # if logdir is configured, idds will write to idds.log in this directory.
            # else idds will go to stdout/stderr.
            # With supervisord, it's good to write to stdout/stderr, then supervisord can manage and rotate logs.
            # logdir = /var/log/idds
            loglevel = INFO
            [rest]
            host = https://iddsserver.cern.ch:443/idds
            #url_prefix = /idds
            #cacher_dir = /tmp
            cacher_dir = /data/idds

    safir:
      # -- Level of Python logging
      logLevel: "INFO"

      # -- Path prefix that will be routed to the controller
      pathPrefix: "/nublado"

# JupyterHub configuration handled directly by this chart rather than by Zero
# to JupyterHub.
hub:
  # -- Whether to use the cluster-internal PostgreSQL server instead of an
  # external server. This is not used directly by the Nublado chart, but
  # controls how the database password is managed.
  internalDatabase: true

  timeout:
    # -- Timeout for the Kubernetes spawn process in seconds. (Allow long
    # enough to pull uncached images if needed.)
    spawn: 600

    # -- Timeout for JupyterLab to start. Currently this sometimes takes over
    # 60 seconds for reasons we don't understand.
    startup: 90

# JupyterHub proxy configuration handled directly by this chart rather than by
# Zero to JupyterHub.
proxy:
  ingress:
    # -- Additional annotations to add to the proxy ingress (also used to talk
    # to JupyterHub and all user labs)
    # @default -- Increase `proxy-read-timeout` and `proxy-send-timeout` to 5m
    annotations:
      nginx.ingress.kubernetes.io/proxy-read-timeout: "300"
      nginx.ingress.kubernetes.io/proxy-send-timeout: "300"

# Configuration for the Zero to JupyterHub subchart.
jupyterhub:
  hub:
    # -- Whether to require metrics requests to be authenticated
    authenticatePrometheus: false

    image:
      # -- Image to use for JupyterHub
      name: ghcr.io/lsst-sqre/rsp-restspawner

      # -- Tag of image to use for JupyterHub
      tag: 0.3.2

    # -- Resource limits and requests
    resources:
      limits:
        cpu: 900m
        memory: 1Gi  # Should support about 200 users

    db:
      # -- Type of database to use
      type: "postgres"

      # -- Database password (not used)
      # @default -- Comes from nublado-secret
      password: "true"

      # -- URL of PostgreSQL server
      # @default -- Use the in-cluster PostgreSQL installed by Phalanx
      url: "postgresql://jovyan@postgres.postgres/jupyterhub"

    # -- Security context for JupyterHub container
    containerSecurityContext:
      runAsUser: 768
      runAsGroup: 768
      allowPrivilegeEscalation: false

    # -- Base URL on which JupyterHub listens
    baseUrl: "/nb"

    # -- Existing secret to use for private keys
    existingSecret: "nublado-secret"

    # -- Additional environment variables to set
    # @default -- Gets `JUPYTERHUB_CRYPT_KEY` from `nublado-secret`
    extraEnv:
      JUPYTERHUB_CRYPT_KEY:
        valueFrom:
          secretKeyRef:
            name: "nublado-secret"
            key: "hub.config.CryptKeeper.keys"

    # -- Additional volumes to make available to JupyterHub
    # @default -- The `hub-config` `ConfigMap` and the Gafaelfawr token
    extraVolumes:
      - name: "hub-config"
        configMap:
          name: "hub-config"
      - name: "hub-gafaelfawr-token"
        secret:
          secretName: "hub-gafaelfawr-token"

    # -- Additional volume mounts for JupyterHub
    # @default -- `hub-config` and the Gafaelfawr token
    extraVolumeMounts:
      - name: "hub-config"
        mountPath: "/usr/local/etc/jupyterhub/jupyterhub_config.d"
      - name: "hub-gafaelfawr-token"
        mountPath: "/etc/gafaelfawr"

    networkPolicy:
      # -- Whether to enable the default `NetworkPolicy` (currently, the
      # upstream one does not work correctly)
      enabled: false

    loadRoles:
      server:
        # -- Default scopes for the user's lab, overridden to allow the lab to
        # delete itself (which we use for our added menu items)
        scopes: ["self"]

  prePuller:
    continuous:
      # -- Whether to run the JupyterHub continuous prepuller (the Nublado
      # controller does its own prepulling)
      enabled: false

    hook:
      # -- Whether to run the JupyterHub hook prepuller (the Nublado
      # controller does its own prepulling)
      enabled: false

  singleuser:
    cloudMetadata:
      # -- Whether to configure iptables to block cloud metadata endpoints.
      # This is unnecessary in our environments (they are blocked by cluster
      # configuration) and thus is disabled to reduce complexity.
      blockWithIptables: false

    # -- Start command for labs
    cmd: "/opt/lsst/software/jupyterlab/runlab.sh"

    # -- Default URL prefix for lab endpoints
    defaultUrl: "/lab"

  proxy:
    service:
      # -- Only expose the proxy to the cluster, overriding the default of
      # exposing the proxy directly to the Internet
      type: ClusterIP

    chp:
      networkPolicy:
        # -- Enable access to the proxy from other namespaces, since we put
        # each user's lab environment in its own namespace
        interNamespaceAccessLabels: accept

        # This currently causes Minikube deployment in GH-actions to fail.
        # We want it sometime but it's not critical; it will help with
        # scale-down
        # pdb:
        #   enabled: true
        #   minAvailable: 1

  # Rather than using the JupyterHub-provided ingress, which requires us to
  # repeat the global host name and manually configure authentication, we
  # instead install our own GafaelfawrIngress.
  ingress:
    # -- Whether to enable the default ingress
    enabled: false

  cull:
    # -- Enable the lab culler.
    enabled: true

    # -- Default idle timeout before the lab is automatically deleted in
    # seconds
    # @default -- 2592000 (30 days)
    timeout: 2592000

    # -- How frequently to check for idle labs in seconds
    # @default -- 600 (10 minutes)
    every: 600

    # -- Whether to log out the server when culling their lab
    users: true

    # -- Whether to remove named servers when culling their lab
    removeNamedServers: true

    # -- Maximum age of a lab regardless of activity
    # @default -- 5184000 (60 days)
    maxAge: 5184000

  scheduling:
    userScheduler:
      # -- Whether the user scheduler should be enabled
      enabled: false

    userPlaceholder:
      # -- Whether to spawn placeholder pods representing fake users to force
      # autoscaling in advance of running out of resources
      enabled: false

# The following will be set by parameters injected by Argo CD and should not
# be set in the individual environment values files.
global:
  # -- Base URL for the environment
  # @default -- Set by Argo CD
  baseUrl: ""

  # -- Host name for ingress
  # @default -- Set by Argo CD
  host: ""

  # -- Base path for Vault secrets
  # @default -- Set by Argo CD
  vaultSecretsPath: ""
